```
cmd/kube-proxy/proxy.go
main(): 程序入口
|
└─  cmd/kube-proxy/app/server.go
    NewProxyCommand(): 解析命令行选项. 
    |   调用NewOptions()函数构造`Options`对象, 设置默认值.
    |   `cobra.Command.Run`成员函数中补全选项参数并校验, 
    |   初始化配置文件变动监听器`Options.watcher`及文件变动的事件处理器(此时并未运行).
    |   最终调用`Options.Run()`, 此函数将一直运行直到程序终止.
    |
    └─  cmd/kube-proxy/app/server.go
        Options.Run(): 创建`proxyServer`, 并开始执行.
        |
        |
        ├─  cmd/kube-proxy/app/server_others.go
        |   NewProxyServer()
        |   |
        |   newProxyServer(): 创建ProxyServer对象, 该对象成员很多, 过程复杂.
        |   |   `ProxyServer`结构实现了`cmd/kube-proxy/app/proxy_server.go` -> proxyRun 接口
        |   |   且创建的ProxyServer直接被赋予给`Options.proxyServer`成员.
        |   |
        |   |   按照预置模型, 调用`iptables`, `ipvs`或`userspace`中的构造函数, 
        |   |   初始化`Proxier`对象, 并赋值给`ProxyServer.Proxier`成员.
        |   |
        |   ├─  
        |   |
        |
        └─  cmd/kube-proxy/app/server.go
            Options.runLoop(): 阻塞进程.
            |   启动监听器监测配置文件变动, 同时执行`proxyServer.Run()`
            |
            ├─  Options.watcher.Run(): 启动配置文件监听器.
            |
            └─  Options.proxyServer.Run(): 正式执行.
                |   这里用`for{}`无限循环等待err事件才会中断.
                |
                └─  cmd/kube-proxy/app/proxy_server.go
                    ProxyServer.Run(): 
                    |   在这个函数中, 代码十分类似于CRD工程, 会创建`informerFactory`,
                    |   以监听service/endpoint的变动.
                    |
                    └─  ProxyServer.Proxier.SyncLoop()
                        |
                        └─  Proxier.syncRunner.Loop(): `syncRunner`成员在`NewProxier()`中初始化.
                            |   其实`syncRunner`在初始化时就注册了一个周期性函数, ta本身只是这个cronjob的句柄而已,
                            |   最终真正执行的是`Proxier.syncProxyRules()`方法.
                            |
                            └─  Proxier.syncProxyRules(): 被周期性调用, 保证系统中的ipvs/iptables规则与svc/endp资源相匹配.
                                |   创建ipvs机制所需的`dummy`设备`kube-ipvs0`.
                                    为新增的service创建virtual server选项, 为新增的endpoint创建real server入口.
                                    同时需要根据service的Type(ClusterIP, NodePort, LoadBalancer)与是否使用`hostNetwork`
                                    添加不同的iptables规则与ipvs条目.

                                    有些Pod被部署在当前节点, 那么对该Pod对应的Endpoint的访问可以放在`KUBE-LOOP-BACK`链中,
                                    如果不是, 则要放到`KUBE-CLUSTER-IP`链.
                                    `NodePort`, `LoadBalancer`也有对应的iptables链, 不过要注意,
                                    这些链都通过`ipset`工具统一管理, 要比直接写`iptables`规则方便很多.
```

TODO:

1. `kube-ipvs0`设备存在的意义是什么? 我在做ipvsadm实验的时候并没有发现需要额外的网络接口.
